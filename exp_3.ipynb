{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Exp:3. Illustrate the performance of various Optimization techniques of Gradient Descent(GD), Momentum Based GD, Nesterov Accelerated GD, Stochastic GD, AdaGrad, RMSProp, Adam\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Define a function to create and compile a model\n",
    "def create_model(optimizer):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define different optimizers\n",
    "optimizers = {\n",
    "    'SGD': tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "    'Momentum': tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    'Nesterov': tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n",
    "    'AdaGrad': tf.keras.optimizers.Adagrad(learning_rate=0.01),\n",
    "    'RMSProp': tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    'Adam': tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store accuracy history for each optimizer\n",
    "accuracy_history = {}\n",
    "\n",
    "# Train and evaluate models with different optimizers\n",
    "num_epochs = 5\n",
    "\n",
    "for optimizer_name, optimizer in optimizers.items():\n",
    "    model = create_model(optimizer)\n",
    "    history = model.fit(X_train, y_train, epochs=num_epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "    accuracy_history[optimizer_name] = history.history['accuracy']\n",
    "\n",
    "# Plot accuracy curves for each optimizer\n",
    "plt.figure(figsize=(10, 6))\n",
    "for optimizer_name, accuracy_values in accuracy_history.items():\n",
    "    plt.plot(accuracy_values, label=optimizer_name)\n",
    "\n",
    "plt.title('Accuracy Curves for Different Optimizers')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
