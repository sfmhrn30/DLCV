{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"18ADAuIITTY3yJAHw1-jAzCVGbGAhLAcQ","timestamp":1716302030738}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense\n","\n","# Sample data (simple sequence for demonstration purposes)\n","# Normally you would use real data here\n","input_texts = [\"hello\", \"world\", \"machine\", \"learning\"]\n","target_texts = [\"\\t\" + text + \"\\n\" for text in [\"hola\", \"mundo\", \"maquina\", \"aprendizaje\"]]\n","\n","# Preprocessing the data\n","input_characters = sorted(list(set(''.join(input_texts))))\n","target_characters = sorted(list(set(''.join(target_texts))))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n","reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n","\n","# Vectorizing the data\n","encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n","decoder_input_data = np.zeros((len(target_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n","decoder_target_data = np.zeros((len(target_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n","\n","for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.\n","    for t, char in enumerate(target_text):\n","        decoder_input_data[i, t, target_token_index[char]] = 1.\n","        if t > 0:\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n","\n","# Define the model\n","latent_dim = 256\n","\n","# Encoder\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder_lstm = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n","encoder_states = [state_h, state_c]\n","\n","# Decoder\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model that will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# Compile & Train the model\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=64, epochs=100, validation_split=0.2)\n","\n","# Define sampling models\n","# Encoder model\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","# Decoder model\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)\n","\n","# Reverse-lookup token index to decode sequences back to something readable\n","reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n","\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character\n","    target_seq[0, 0, target_token_index['\\t']] = 1.\n","\n","    # Sampling loop for a batch of sequences\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length or find stop character\n","        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n","            stop_condition = True\n","\n","        # Update the target sequence (length 1)\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence\n","\n","# Testing with a simple example\n","for seq_index in range(len(input_texts)):\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVVNEHDULTg8","executionInfo":{"status":"ok","timestamp":1715758899422,"user_tz":-330,"elapsed":19412,"user":{"displayName":"Sreenesh Reddy","userId":"17967487360557110091"}},"outputId":"756340b6-1487-4597-8beb-fc296313d538"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","1/1 [==============================] - 4s 4s/step - loss: 1.3817 - accuracy: 0.0256 - val_loss: 2.6056 - val_accuracy: 0.0769\n","Epoch 2/100\n","1/1 [==============================] - 0s 111ms/step - loss: 1.3685 - accuracy: 0.1795 - val_loss: 2.6048 - val_accuracy: 0.1538\n","Epoch 3/100\n","1/1 [==============================] - 0s 102ms/step - loss: 1.3575 - accuracy: 0.2051 - val_loss: 2.6042 - val_accuracy: 0.1538\n","Epoch 4/100\n","1/1 [==============================] - 0s 112ms/step - loss: 1.3466 - accuracy: 0.2308 - val_loss: 2.6037 - val_accuracy: 0.0769\n","Epoch 5/100\n","1/1 [==============================] - 0s 109ms/step - loss: 1.3348 - accuracy: 0.2308 - val_loss: 2.6034 - val_accuracy: 0.0769\n","Epoch 6/100\n","1/1 [==============================] - 0s 113ms/step - loss: 1.3206 - accuracy: 0.1795 - val_loss: 2.6039 - val_accuracy: 0.1538\n","Epoch 7/100\n","1/1 [==============================] - 0s 125ms/step - loss: 1.3013 - accuracy: 0.1282 - val_loss: 2.6124 - val_accuracy: 0.1538\n","Epoch 8/100\n","1/1 [==============================] - 0s 125ms/step - loss: 1.2716 - accuracy: 0.1026 - val_loss: 2.6811 - val_accuracy: 0.1538\n","Epoch 9/100\n","1/1 [==============================] - 0s 108ms/step - loss: 1.2263 - accuracy: 0.1026 - val_loss: 2.8080 - val_accuracy: 0.1538\n","Epoch 10/100\n","1/1 [==============================] - 0s 124ms/step - loss: 1.1892 - accuracy: 0.1026 - val_loss: 2.8598 - val_accuracy: 0.1538\n","Epoch 11/100\n","1/1 [==============================] - 0s 134ms/step - loss: 1.1657 - accuracy: 0.1026 - val_loss: 2.9079 - val_accuracy: 0.1538\n","Epoch 12/100\n","1/1 [==============================] - 0s 115ms/step - loss: 1.1412 - accuracy: 0.1026 - val_loss: 2.9637 - val_accuracy: 0.1538\n","Epoch 13/100\n","1/1 [==============================] - 0s 109ms/step - loss: 1.1233 - accuracy: 0.1026 - val_loss: 3.0039 - val_accuracy: 0.0769\n","Epoch 14/100\n","1/1 [==============================] - 0s 107ms/step - loss: 1.1008 - accuracy: 0.1282 - val_loss: 3.0293 - val_accuracy: 0.0769\n","Epoch 15/100\n","1/1 [==============================] - 0s 73ms/step - loss: 1.0832 - accuracy: 0.1538 - val_loss: 3.0480 - val_accuracy: 0.0769\n","Epoch 16/100\n","1/1 [==============================] - 0s 70ms/step - loss: 1.0644 - accuracy: 0.2051 - val_loss: 3.0756 - val_accuracy: 0.0769\n","Epoch 17/100\n","1/1 [==============================] - 0s 95ms/step - loss: 1.0421 - accuracy: 0.1795 - val_loss: 3.0997 - val_accuracy: 0.0769\n","Epoch 18/100\n","1/1 [==============================] - 0s 68ms/step - loss: 1.0314 - accuracy: 0.1538 - val_loss: 3.1257 - val_accuracy: 0.0769\n","Epoch 19/100\n","1/1 [==============================] - 0s 70ms/step - loss: 1.0074 - accuracy: 0.2051 - val_loss: 3.1543 - val_accuracy: 0.0769\n","Epoch 20/100\n","1/1 [==============================] - 0s 74ms/step - loss: 0.9981 - accuracy: 0.1538 - val_loss: 3.1812 - val_accuracy: 0.0769\n","Epoch 21/100\n","1/1 [==============================] - 0s 73ms/step - loss: 0.9763 - accuracy: 0.1795 - val_loss: 3.1848 - val_accuracy: 0.0769\n","Epoch 22/100\n","1/1 [==============================] - 0s 72ms/step - loss: 0.9696 - accuracy: 0.1538 - val_loss: 3.2126 - val_accuracy: 0.0769\n","Epoch 23/100\n","1/1 [==============================] - 0s 71ms/step - loss: 0.9596 - accuracy: 0.1538 - val_loss: 3.2427 - val_accuracy: 0.0769\n","Epoch 24/100\n","1/1 [==============================] - 0s 87ms/step - loss: 0.9400 - accuracy: 0.2051 - val_loss: 3.2249 - val_accuracy: 0.0769\n","Epoch 25/100\n","1/1 [==============================] - 0s 70ms/step - loss: 0.9407 - accuracy: 0.1538 - val_loss: 3.2861 - val_accuracy: 0.0769\n","Epoch 26/100\n","1/1 [==============================] - 0s 69ms/step - loss: 0.9148 - accuracy: 0.2308 - val_loss: 3.2332 - val_accuracy: 0.0769\n","Epoch 27/100\n","1/1 [==============================] - 0s 74ms/step - loss: 0.9322 - accuracy: 0.2051 - val_loss: 3.2996 - val_accuracy: 0.1538\n","Epoch 28/100\n","1/1 [==============================] - 0s 71ms/step - loss: 0.8992 - accuracy: 0.1795 - val_loss: 3.2458 - val_accuracy: 0.0769\n","Epoch 29/100\n","1/1 [==============================] - 0s 79ms/step - loss: 0.9193 - accuracy: 0.1795 - val_loss: 3.3349 - val_accuracy: 0.1538\n","Epoch 30/100\n","1/1 [==============================] - 0s 96ms/step - loss: 0.8882 - accuracy: 0.1795 - val_loss: 3.2802 - val_accuracy: 0.0769\n","Epoch 31/100\n","1/1 [==============================] - 0s 71ms/step - loss: 0.8993 - accuracy: 0.2308 - val_loss: 3.3730 - val_accuracy: 0.1538\n","Epoch 32/100\n","1/1 [==============================] - 0s 72ms/step - loss: 0.8749 - accuracy: 0.2308 - val_loss: 3.2198 - val_accuracy: 0.0769\n","Epoch 33/100\n","1/1 [==============================] - 0s 88ms/step - loss: 0.9146 - accuracy: 0.2051 - val_loss: 3.3484 - val_accuracy: 0.0769\n","Epoch 34/100\n","1/1 [==============================] - 0s 88ms/step - loss: 0.8720 - accuracy: 0.2308 - val_loss: 3.3608 - val_accuracy: 0.0769\n","Epoch 35/100\n","1/1 [==============================] - 0s 94ms/step - loss: 0.8578 - accuracy: 0.2051 - val_loss: 3.3688 - val_accuracy: 0.0769\n","Epoch 36/100\n","1/1 [==============================] - 0s 70ms/step - loss: 0.8516 - accuracy: 0.2308 - val_loss: 3.3746 - val_accuracy: 0.0769\n","Epoch 37/100\n","1/1 [==============================] - 0s 73ms/step - loss: 0.8398 - accuracy: 0.2051 - val_loss: 3.3626 - val_accuracy: 0.0769\n","Epoch 38/100\n","1/1 [==============================] - 0s 71ms/step - loss: 0.8498 - accuracy: 0.1795 - val_loss: 3.4113 - val_accuracy: 0.1538\n","Epoch 39/100\n","1/1 [==============================] - 0s 71ms/step - loss: 0.8279 - accuracy: 0.2051 - val_loss: 3.2600 - val_accuracy: 0.0769\n","Epoch 40/100\n","1/1 [==============================] - 0s 69ms/step - loss: 0.8884 - accuracy: 0.1795 - val_loss: 3.4249 - val_accuracy: 0.0769\n","Epoch 41/100\n","1/1 [==============================] - 0s 77ms/step - loss: 0.8367 - accuracy: 0.2051 - val_loss: 3.4000 - val_accuracy: 0.0769\n","Epoch 42/100\n","1/1 [==============================] - 0s 75ms/step - loss: 0.8216 - accuracy: 0.1795 - val_loss: 3.4458 - val_accuracy: 0.1538\n","Epoch 43/100\n","1/1 [==============================] - 0s 85ms/step - loss: 0.8106 - accuracy: 0.2051 - val_loss: 3.3023 - val_accuracy: 0.0769\n","Epoch 44/100\n","1/1 [==============================] - 0s 71ms/step - loss: 0.8747 - accuracy: 0.2051 - val_loss: 3.4313 - val_accuracy: 0.0769\n","Epoch 45/100\n","1/1 [==============================] - 0s 71ms/step - loss: 0.7984 - accuracy: 0.2051 - val_loss: 3.4275 - val_accuracy: 0.1538\n","Epoch 46/100\n","1/1 [==============================] - 0s 86ms/step - loss: 0.7875 - accuracy: 0.2051 - val_loss: 3.4354 - val_accuracy: 0.0769\n","Epoch 47/100\n","1/1 [==============================] - 0s 66ms/step - loss: 0.7714 - accuracy: 0.2564 - val_loss: 3.3728 - val_accuracy: 0.0769\n","Epoch 48/100\n","1/1 [==============================] - 0s 75ms/step - loss: 0.8033 - accuracy: 0.2051 - val_loss: 3.4645 - val_accuracy: 0.0769\n","Epoch 49/100\n","1/1 [==============================] - 0s 73ms/step - loss: 0.8114 - accuracy: 0.2051 - val_loss: 3.2188 - val_accuracy: 0.0769\n","Epoch 50/100\n","1/1 [==============================] - 0s 80ms/step - loss: 0.9172 - accuracy: 0.1538 - val_loss: 3.2921 - val_accuracy: 0.0769\n","Epoch 51/100\n","1/1 [==============================] - 0s 74ms/step - loss: 0.8411 - accuracy: 0.2308 - val_loss: 3.3945 - val_accuracy: 0.0769\n","Epoch 52/100\n","1/1 [==============================] - 0s 84ms/step - loss: 0.7850 - accuracy: 0.2821 - val_loss: 3.4415 - val_accuracy: 0.0769\n","Epoch 53/100\n","1/1 [==============================] - 0s 76ms/step - loss: 0.7576 - accuracy: 0.3077 - val_loss: 3.4563 - val_accuracy: 0.0769\n","Epoch 54/100\n","1/1 [==============================] - 0s 72ms/step - loss: 0.7369 - accuracy: 0.3077 - val_loss: 3.4512 - val_accuracy: 0.0769\n","Epoch 55/100\n","1/1 [==============================] - 0s 66ms/step - loss: 0.7171 - accuracy: 0.3077 - val_loss: 3.4628 - val_accuracy: 0.1538\n","Epoch 56/100\n","1/1 [==============================] - 0s 91ms/step - loss: 0.7000 - accuracy: 0.3333 - val_loss: 3.4419 - val_accuracy: 0.0769\n","Epoch 57/100\n","1/1 [==============================] - 0s 69ms/step - loss: 0.6864 - accuracy: 0.3077 - val_loss: 3.4466 - val_accuracy: 0.0769\n","Epoch 58/100\n","1/1 [==============================] - 0s 71ms/step - loss: 0.6763 - accuracy: 0.3077 - val_loss: 3.4524 - val_accuracy: 0.1538\n","Epoch 59/100\n","1/1 [==============================] - 0s 69ms/step - loss: 0.6886 - accuracy: 0.2564 - val_loss: 3.4668 - val_accuracy: 0.0769\n","Epoch 60/100\n","1/1 [==============================] - 0s 70ms/step - loss: 0.6839 - accuracy: 0.2821 - val_loss: 3.4216 - val_accuracy: 0.0769\n","Epoch 61/100\n","1/1 [==============================] - 0s 69ms/step - loss: 0.7380 - accuracy: 0.2308 - val_loss: 3.4962 - val_accuracy: 0.0769\n","Epoch 62/100\n","1/1 [==============================] - 0s 70ms/step - loss: 0.6736 - accuracy: 0.2564 - val_loss: 3.2867 - val_accuracy: 0.0769\n","Epoch 63/100\n","1/1 [==============================] - 0s 74ms/step - loss: 0.8007 - accuracy: 0.1795 - val_loss: 3.3794 - val_accuracy: 0.0769\n","Epoch 64/100\n","1/1 [==============================] - 0s 70ms/step - loss: 0.6561 - accuracy: 0.3590 - val_loss: 3.4821 - val_accuracy: 0.0769\n","Epoch 65/100\n","1/1 [==============================] - 0s 69ms/step - loss: 0.6074 - accuracy: 0.2821 - val_loss: 3.4027 - val_accuracy: 0.0769\n","Epoch 66/100\n","1/1 [==============================] - 0s 70ms/step - loss: 0.6188 - accuracy: 0.3846 - val_loss: 3.4981 - val_accuracy: 0.1538\n","Epoch 67/100\n","1/1 [==============================] - 0s 71ms/step - loss: 0.5853 - accuracy: 0.2821 - val_loss: 3.3453 - val_accuracy: 0.0769\n","Epoch 68/100\n","1/1 [==============================] - 0s 67ms/step - loss: 0.6587 - accuracy: 0.3077 - val_loss: 3.4899 - val_accuracy: 0.1538\n","Epoch 69/100\n","1/1 [==============================] - 0s 67ms/step - loss: 0.5597 - accuracy: 0.3077 - val_loss: 3.4243 - val_accuracy: 0.0769\n","Epoch 70/100\n","1/1 [==============================] - 0s 79ms/step - loss: 0.5730 - accuracy: 0.4103 - val_loss: 3.5206 - val_accuracy: 0.1538\n","Epoch 71/100\n","1/1 [==============================] - 0s 84ms/step - loss: 0.5545 - accuracy: 0.2821 - val_loss: 3.3257 - val_accuracy: 0.0769\n","Epoch 72/100\n","1/1 [==============================] - 0s 72ms/step - loss: 0.6542 - accuracy: 0.3333 - val_loss: 3.4553 - val_accuracy: 0.0769\n","Epoch 73/100\n","1/1 [==============================] - 0s 70ms/step - loss: 0.5320 - accuracy: 0.4103 - val_loss: 3.5086 - val_accuracy: 0.0769\n","Epoch 74/100\n","1/1 [==============================] - 0s 74ms/step - loss: 0.5170 - accuracy: 0.3846 - val_loss: 3.4510 - val_accuracy: 0.0769\n","Epoch 75/100\n","1/1 [==============================] - 0s 74ms/step - loss: 0.5153 - accuracy: 0.3590 - val_loss: 3.5615 - val_accuracy: 0.1538\n","Epoch 76/100\n","1/1 [==============================] - 0s 70ms/step - loss: 0.5285 - accuracy: 0.3590 - val_loss: 3.4153 - val_accuracy: 0.0769\n","Epoch 77/100\n","1/1 [==============================] - 0s 70ms/step - loss: 0.5391 - accuracy: 0.3590 - val_loss: 3.5635 - val_accuracy: 0.1538\n","Epoch 78/100\n","1/1 [==============================] - 0s 86ms/step - loss: 0.5037 - accuracy: 0.3590 - val_loss: 3.4419 - val_accuracy: 0.0769\n","Epoch 79/100\n","1/1 [==============================] - 0s 73ms/step - loss: 0.5087 - accuracy: 0.3846 - val_loss: 3.5844 - val_accuracy: 0.1538\n","Epoch 80/100\n","1/1 [==============================] - 0s 71ms/step - loss: 0.5004 - accuracy: 0.2821 - val_loss: 3.3591 - val_accuracy: 0.0769\n","Epoch 81/100\n","1/1 [==============================] - 0s 68ms/step - loss: 0.6651 - accuracy: 0.2821 - val_loss: 3.4485 - val_accuracy: 0.0769\n","Epoch 82/100\n","1/1 [==============================] - 0s 72ms/step - loss: 0.4959 - accuracy: 0.3333 - val_loss: 3.5548 - val_accuracy: 0.1538\n","Epoch 83/100\n","1/1 [==============================] - 0s 80ms/step - loss: 0.4868 - accuracy: 0.3846 - val_loss: 3.5065 - val_accuracy: 0.0769\n","Epoch 84/100\n","1/1 [==============================] - 0s 76ms/step - loss: 0.4666 - accuracy: 0.4103 - val_loss: 3.5531 - val_accuracy: 0.1538\n","Epoch 85/100\n","1/1 [==============================] - 0s 73ms/step - loss: 0.4662 - accuracy: 0.4103 - val_loss: 3.5363 - val_accuracy: 0.0769\n","Epoch 86/100\n","1/1 [==============================] - 0s 70ms/step - loss: 0.4494 - accuracy: 0.3846 - val_loss: 3.5380 - val_accuracy: 0.1538\n","Epoch 87/100\n","1/1 [==============================] - 0s 91ms/step - loss: 0.4761 - accuracy: 0.4103 - val_loss: 3.5891 - val_accuracy: 0.0769\n","Epoch 88/100\n","1/1 [==============================] - 0s 68ms/step - loss: 0.4571 - accuracy: 0.3077 - val_loss: 3.4075 - val_accuracy: 0.0769\n","Epoch 89/100\n","1/1 [==============================] - 0s 70ms/step - loss: 0.5976 - accuracy: 0.3333 - val_loss: 3.5561 - val_accuracy: 0.0769\n","Epoch 90/100\n","1/1 [==============================] - 0s 71ms/step - loss: 0.4467 - accuracy: 0.3590 - val_loss: 3.4755 - val_accuracy: 0.0769\n","Epoch 91/100\n","1/1 [==============================] - 0s 72ms/step - loss: 0.4664 - accuracy: 0.4103 - val_loss: 3.6136 - val_accuracy: 0.1538\n","Epoch 92/100\n","1/1 [==============================] - 0s 71ms/step - loss: 0.4368 - accuracy: 0.3077 - val_loss: 3.4106 - val_accuracy: 0.0769\n","Epoch 93/100\n","1/1 [==============================] - 0s 87ms/step - loss: 0.5432 - accuracy: 0.3846 - val_loss: 3.5724 - val_accuracy: 0.0769\n","Epoch 94/100\n","1/1 [==============================] - 0s 68ms/step - loss: 0.4337 - accuracy: 0.3333 - val_loss: 3.5216 - val_accuracy: 0.0769\n","Epoch 95/100\n","1/1 [==============================] - 0s 70ms/step - loss: 0.4750 - accuracy: 0.4359 - val_loss: 3.6122 - val_accuracy: 0.1538\n","Epoch 96/100\n","1/1 [==============================] - 0s 78ms/step - loss: 0.4304 - accuracy: 0.3077 - val_loss: 3.4467 - val_accuracy: 0.0769\n","Epoch 97/100\n","1/1 [==============================] - 0s 72ms/step - loss: 0.5048 - accuracy: 0.4103 - val_loss: 3.6009 - val_accuracy: 0.0769\n","Epoch 98/100\n","1/1 [==============================] - 0s 66ms/step - loss: 0.4146 - accuracy: 0.3590 - val_loss: 3.5033 - val_accuracy: 0.0769\n","Epoch 99/100\n","1/1 [==============================] - 0s 68ms/step - loss: 0.4617 - accuracy: 0.4615 - val_loss: 3.6206 - val_accuracy: 0.1538\n","Epoch 100/100\n","1/1 [==============================] - 0s 72ms/step - loss: 0.4163 - accuracy: 0.3333 - val_loss: 3.4540 - val_accuracy: 0.0769\n","1/1 [==============================] - 0s 355ms/step\n","1/1 [==============================] - 0s 373ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 21ms/step\n","-\n","Input sentence: hello\n","Decoded sentence: hola\n","\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 20ms/step\n","-\n","Input sentence: world\n","Decoded sentence: mund\n","\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 29ms/step\n","-\n","Input sentence: machine\n","Decoded sentence: maqunn\n","\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 22ms/step\n","-\n","Input sentence: learning\n","Decoded sentence: mun\n","\n"]}]}]}